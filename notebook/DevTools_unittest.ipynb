{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Unit testing framework\n",
    "\n",
    "## 26.4  unittest — Unit testing framework\n",
    "\n",
    "https://docs.python.org/3/library/unittest.html\n",
    "\n",
    "The unittest unit testing framework was originally inspired by **JUnit** \n",
    "\n",
    "and has a similar flavor as major unit testing frameworks in other languages. \n",
    "\n",
    "It supports test automation, sharing of setup and shutdown code for tests, aggregation of tests into collections, and independence of the tests from the reporting framework.\n",
    "\n",
    "To achieve this, **unittest** supports some important concepts in an object-oriented way:\n",
    "\n",
    "* <b>test fixture</b>: \n",
    "\n",
    "  * A test fixture represents the **preparation** needed to perform one or more tests, and any associate **cleanup actions**. \n",
    "  \n",
    "  This may involve, for example, creating temporary or proxy databases, directories, or starting a server process.\n",
    "\n",
    "\n",
    "* <b>test case</b>: \n",
    "\n",
    "   * A test case is the **individual** unit of testing. \n",
    "   \n",
    "   It checks for a **specific** response to a **particular** set of inputs.\n",
    "   \n",
    "   **unittest** provides a base class,　**TestCase**, which may be used to create new test cases.\n",
    "\n",
    "\n",
    "* <b>test suite</b>: \n",
    "\n",
    "  * A test suite is a **collection** of test cases, test suites, or both.\n",
    "  \n",
    "  It is used to **aggregate tests** that should be executed together.\n",
    "\n",
    "\n",
    "* <b>test runner</b>: \n",
    "\n",
    "  * A test runner is a component which orchestrates the execution of tests and provides the outcome to the user. \n",
    "  \n",
    "  The runner may use a graphical interface, a textual interface, or return a special value to indicate the results of executing the tests\n",
    "\n",
    "\n",
    "## Basic Test Structure\n",
    "\n",
    "Tests, as defined by <b>unittest</b>, have two parts: \n",
    "\n",
    "* <b>code to manage test “fixtures”</b>\n",
    "\n",
    "* <b>the test itself</b>. \n",
    "\n",
    "**Individual tests** are created by \n",
    "\n",
    "* subclassing **TestCase** \n",
    "\n",
    "* overriding or adding appropriate methods \n",
    "\n",
    "For example: \n",
    "\n",
    "* **unittest_simple.py**        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%file ./code/unittest/unittest_simple.py\n",
    "\n",
    "import unittest\n",
    "\n",
    "class SimplisticTest(unittest.TestCase):\n",
    "\n",
    "    def test_true(self):\n",
    "        self.assertTrue(True)\n",
    " \n",
    "    def test(self):\n",
    "        self.assertTrue(True)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    unittest.main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this case, the <b>SimplisticTest</b> have \n",
    "\n",
    "* <b>test_true()</b> \n",
    "\n",
    "* <b>test()</b>\n",
    "\n",
    "methods would <b>fail if True is ever False</b>.\n",
    "\n",
    "The methods are defined with name \n",
    "\n",
    "* **start** with the letters **test**. \n",
    "\n",
    "This naming convention informs the <b>test runner</b> about which methods represent tests.\n",
    "\n",
    "## Running Tests\n",
    "\n",
    "The easiest way to run unittest tests is to include:\n",
    "```python\n",
    "    if __name__ == '__main__':\n",
    "        unittest.main()\n",
    "```\n",
    "at the bottom of each test file, then simply run the script directly from the command line:\n",
    "```    \n",
    "   >python unittest_simple.py\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python ./code/unittest/unittest_simple.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run ./code/unittest/unittest_simple.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "includes \n",
    "\n",
    "* <b>a status indicator for each test</b> \n",
    "\n",
    "   * **”.”** on the first line of output means that a test <b>passed<b>\n",
    "\n",
    "* <b>the amount of time the tests took</b>, \n",
    "\n",
    "\n",
    "For **more** detailed test</b> results, \n",
    "\n",
    "**-v** option:\n",
    "\n",
    "```\n",
    ">python unittest_simple.py -v\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run ./code/unittest/unittest_simple.py -v"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Outcomes\n",
    "\n",
    "Tests have 3 possible outcomes:\n",
    "\n",
    "* <b>ok -　　 **.**　　</b>: The test passes \n",
    "\n",
    "* <b>FAIL -　　 **F**　　</b>:The test does not pass, and raises an **AssertionError** exception. \n",
    "\n",
    "* <b>ERROR - 　　**E**　　</b>: The test raises an **exception** other than AssertionError.\n",
    "\n",
    "a test’s status depends on the presence (or absence) of an exception.\n",
    "\n",
    "For Example: \n",
    "\n",
    "* **unittest_outcomes.py**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%file ./code/unittest/unittest_outcomes.py\n",
    "\n",
    "import unittest\n",
    "\n",
    "class OutcomesTest(unittest.TestCase):\n",
    "\n",
    "    #   ok\n",
    "    def test_Pass(self):\n",
    "        return\n",
    "\n",
    "    # FAIL\n",
    "    def test_Fail(self):\n",
    "        # AssertionError exception.\n",
    "        self.assertFalse(True)\n",
    "        \n",
    "    # ERROR\n",
    "    def test_Error(self):\n",
    "        # raises an exception other than AssertionError\n",
    "        raise RuntimeError('Test error!')\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    unittest.main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run ./code/unittest/unittest_outcomes.py "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#When a test fails or generates an error, \n",
    "\n",
    "the <b>traceback</b> is included in the output.\n",
    "In the example above, \n",
    "\n",
    "<b>testFail()</b> fails and the traceback <b>shows the line</b> with the failure code.\n",
    "\n",
    "It is up to the person reading the test output to look at the code to figure out the semantic meaning of the failed test, though. \n",
    "\n",
    "### fail with message\n",
    "\n",
    "To make it <b>easier to understand the nature of a test failure</b>,\n",
    "\n",
    "the <b>fail*() and assert*()</b> methods all accept an argument <b>msg</b>,\n",
    "\n",
    "which can be used to produce <b>a more detailed error message</b>\n",
    "\n",
    "Example: \n",
    "\n",
    "* **unittest_failwithmessage.py**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%file ./code/unittest/unittest_failwithmessage.py\n",
    "\n",
    "import unittest\n",
    "\n",
    "class FailureMessageTest(unittest.TestCase):\n",
    "\n",
    "    def test_Fail(self):\n",
    "        self.assertFalse(True,'failure message goes here')\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    unittest.main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run ./code/unittest/unittest_failwithmessage.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Asserting Truth\n",
    "\n",
    "Most tests assert the truth of some condition. There are a few different ways to write truth-checking tests, depending on the perspective of the test author and the desired outcome of the code being tested. \n",
    "\n",
    "If the code produces a value which can be evaluated as <b>true</b>, the methods <b>assertTrue()</b>  should be used.\n",
    "\n",
    "If the code produces a <b>false</b> value, the methods <b>assertFalse()</b> make more sense."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%file ./code/unittest/unittest_true.py\n",
    "\n",
    "import unittest\n",
    "\n",
    "class TruthTest(unittest.TestCase):\n",
    "\n",
    "    def testAssertTrue(self):\n",
    "        self.assertTrue(True)\n",
    "\n",
    "    def test_AssertFalse(self):\n",
    "        self.assertFalse(False)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    unittest.main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run ./code/unittest/unittest_true.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing Equality\n",
    "\n",
    "As a special case, `unittest` includes methods for testing <b>the equality of two values</b>.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%file ./code/unittest/unittest_equality.py\n",
    "\n",
    "import unittest\n",
    "\n",
    "class EqualityTest(unittest.TestCase):\n",
    "\n",
    "    def test_Equal(self):\n",
    "        self.assertEqual(1, 3)\n",
    "\n",
    "    def test_NotEqual(self):\n",
    "        self.assertNotEqual(2, 3-2)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    unittest.main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run ./code/unittest/unittest_equality.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These special tests are handy, since the values being <b>compared appear in the failure message</b> when a test fails."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%file ./code/unittest/unittest_notequal.py\n",
    "import unittest\n",
    "\n",
    "class InequalityTest(unittest.TestCase):\n",
    "\n",
    "    def test_Equal(self):\n",
    "        self.assertNotEqual(1, 3-2)\n",
    "\n",
    "    def test_NotEqual(self):\n",
    "        self.assertEqual(2, 3-2)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    unittest.main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run ./code/unittest/unittest_notequal.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Almost Equal?\n",
    "\n",
    "In addition to strict equality, it is possible to test for\n",
    "\n",
    "**near equality of floating point numbers** using\n",
    "\n",
    "* assertNotAlmostEqual()\n",
    "\n",
    "* assertAlmostEqual()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%file ./code/unittest/unittest_almostequal.py\n",
    "import unittest\n",
    "\n",
    "class AlmostEqualTest(unittest.TestCase):\n",
    "\n",
    "    def test_NotAlmostEqual(self):\n",
    "        self.assertNotAlmostEqual(1.11, 3.3-2.0, places=1)\n",
    "\n",
    "    def test_AlmostEqual(self):\n",
    "        self.assertAlmostEqual(1.1, 3.3-2.0, places=0)\n",
    "\n",
    "    def test_AlmostEqualWithDEfault(self):\n",
    "        self.assertNotAlmostEqual(1.1, 3.3-2.0)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    unittest.main()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The arguments are the values to be compared, and <b>the number of decimal places</b> to use for the test.\n",
    "\n",
    "assertAlmostEquals() and assertNotAlmostEqual  have an optional parameter named\n",
    "\n",
    "<b>places</b> \n",
    "\n",
    "and the numbers are compared by <b>computing the difference rounded to number of decimal places</b>.\n",
    "\n",
    "default <b>places=7</b>,\n",
    "\n",
    "hence:\n",
    "\n",
    "```python\n",
    "self.assertAlmostEqual(0.5, 0.4) is False \n",
    "```\n",
    "\n",
    "```python\n",
    "self.assertAlmostEqual(0.12345678, 0.12345679) is True.\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run ./code/unittest/unittest_almostequal.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing for Exceptions\n",
    "\n",
    "If **a test raises an exception** other than **AssertionError*> it is treated as an error. \n",
    "\n",
    "This is very useful for \n",
    "\n",
    "<b>uncovering mistakes</b>\n",
    "\n",
    "while you are modifying code which has existing test coverage. \n",
    "\n",
    "There are circumstances, however, in which you want the test to verify that some code does produce an exception.\n",
    "\n",
    "For example, if an invalid value is given to an attribute of an object. \n",
    "\n",
    "In such cases, \n",
    "\n",
    "```python\n",
    "assertRaises()\n",
    "```\n",
    "makes the code more clear than trapping the exception yourself. \n",
    "\n",
    "Compare these two tests:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%file ./code/unittest/unittest_exception.py\n",
    "\n",
    "import unittest\n",
    "\n",
    "def raises_error(*args, **kwds):\n",
    "    print(args, kwds) # *args: tuple; **kwds: dict\n",
    "    raise ValueError('Invalid value: ' + str(args) + str(kwds))\n",
    "\n",
    "class ExceptionTest(unittest.TestCase):\n",
    "\n",
    "    def test_TrapLocally(self):\n",
    "        try:\n",
    "            raises_error('a', b='c')\n",
    "        except ValueError:\n",
    "            pass\n",
    "        else:\n",
    "            self.fail('Did not see ValueError')\n",
    "\n",
    "    def test_assertRaises(self):\n",
    "        self.assertRaises(ValueError, raises_error, 'a', b='c')\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    unittest.main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run ./code/unittest/unittest_exception.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "The results for both are the same,\n",
    "\n",
    "but the second test using `assertRaises()` is more succinct."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Note\n",
    "\n",
    "```\n",
    "*args : tuple\n",
    "\n",
    "**kwds: dict\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test Fixtures\n",
    "\n",
    "<b>Fixtures are resources needed by a test</b> \n",
    "\n",
    "* if you are writing several tests for the same class, those tests all need <b>an instance of that class</b> to use for testing. \n",
    "\n",
    "* test fixtures include database connections and temporary files (many people would argue that using external resources makes such tests not “unit” tests, but they are still tests and still useful). \n",
    "\n",
    "** `TestCase` ** includes a special hook to **configure** and **clean up** any fixtures needed by your tests.\n",
    "\n",
    "* To configure the `fixtures`, override `setUp()`.\n",
    "\n",
    "* To clean up, override `tearDown()`.\n",
    "\n",
    "### setUp()\n",
    "\n",
    "Method called to prepare the test fixture. This is called immediately before calling the test method; \n",
    "\n",
    "other than AssertionError or SkipTest, any exception raised by this method will be considered an error rather than a test failure. \n",
    "\n",
    "The default implementation does nothing.\n",
    "\n",
    "### tearDown()\n",
    "\n",
    "Method called immediately after the test method has been called and the result recorded.\n",
    "\n",
    "This is called even if the test method raised an exception, so the implementation in subclasses may need to be particularly careful about checking internal state.\n",
    "\n",
    "Any exception, other than AssertionError or SkipTest, raised by this method will be considered an error rather than a test failure. \n",
    "\n",
    "This method will only be called if the setUp() succeeds, regardless of the outcome of the test method. \n",
    "\n",
    "The default implementation does nothing.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%file ./code/unittest/unittest_fixtures.py\n",
    "\n",
    "import unittest\n",
    "\n",
    "class FixturesTest(unittest.TestCase):\n",
    "\n",
    "    def setUp(self):\n",
    "        print('In setUp()')\n",
    "        self.fixture = range(1, 10)\n",
    "\n",
    "    def tearDown(self):\n",
    "        print('In tearDown()')\n",
    "        del self.fixture\n",
    "\n",
    "    def test_fixture(self):\n",
    "        print('in test()')\n",
    "        self.assertEqual(self.fixture, range(1, 10))\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    unittest.main()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When this sample test is run, you can see \n",
    "\n",
    "* <b>the order of execution</b> of the fixture and test methods:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run ./code/unittest/unittest_fixtures.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 26.4.4. Organizing test code\n",
    "\n",
    "The basic building blocks of unit testing are <b>test cases</b>\n",
    "\n",
    "single scenarios that must be set up and checked for correctness. \n",
    "\n",
    "In unittest, test cases are represented by `unittest.TestCase` instances. \n",
    "\n",
    "To make your own test cases you must write subclasses of TestCase or use FunctionTestCase.\n",
    "\n",
    "The testing code of a TestCase instance should be entirely self contained, such that it can be run either in isolation or in arbitrary combination with any number of other test cases.\n",
    "\n",
    "The simplest TestCase subclass will simply implement a test method\n",
    "\n",
    "* a method whose name starts with **test**\n",
    "\n",
    "in order to perform specific testing code:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%file ./code/unittest/test_DefaultWidgetSizeTestCase.py\n",
    "\n",
    "import unittest\n",
    "\n",
    "class Widget():\n",
    "    def __init__(self, name,size = (40, 40)):\n",
    "        self._name=name\n",
    "        self._size =size\n",
    "\n",
    "    def name(self):\n",
    "        return self._name\n",
    "\n",
    "    def size(self):\n",
    "        return self._size\n",
    "\n",
    "    def resize(self, width, height):\n",
    "        if width < 0  or height < 0:\n",
    "            raise(ValueError, \"illegal size\")\n",
    "        self._size = (width, height)\n",
    "\n",
    "    def dispose(self):\n",
    "        pass\n",
    "\n",
    "class DefaultWidgetSizeTestCase(unittest.TestCase):\n",
    "    \n",
    "    def test_default_widget_size(self):\n",
    "        \n",
    "        widget = Widget('The widget')\n",
    "        \n",
    "        self.assertEqual(widget.size(), (50, 50))\n",
    "        \n",
    "if __name__ == '__main__':\n",
    "    unittest.main()        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run ./code/unittest/test_DefaultWidgetSizeTestCase.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Note that in order to test something, \n",
    "\n",
    "  we use one of the <b>assert*()</b> methods provided by the TestCase base class.\n",
    "  \n",
    "  If the test fails, an exception will be raised, and unittest will identify the test case as a failure.\n",
    "  \n",
    "  Any other exceptions will be treated as errors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tests can be numerous, and their set-up can be repetitive. \n",
    "\n",
    "Luckily, we can factor out set-up code by implementing a method called <b>setUp()</b>, which the testing framework will automatically call for every single test we run:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import unittest\n",
    "\n",
    "class SimpleWidgetTestCase(unittest.TestCase):\n",
    "   \n",
    "    def setUp(self):\n",
    "        self.widget = Widget('The widget')\n",
    "\n",
    "    def test_default_widget_size(self):\n",
    "        self.assertEqual(self.widget.size(), (50,50),\n",
    "                         'incorrect default size')\n",
    "\n",
    "    def test_widget_resize(self):\n",
    "        self.widget.resize(100,150)\n",
    "        self.assertEqual(self.widget.size(), (100,150),\n",
    "                         'wrong size after resize')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">Note:  The order in which the various tests will be run is determined by sorting the test method names with respect to the built-in ordering for strings.\n",
    " \n",
    "\n",
    "If the `setUp()` method raises an exception while the test is running, the framework will consider the test to have suffered an error, and the test method will not be executed.\n",
    "\n",
    "Similarly, we can provide a `tearDown()` method that tidies up after the test method has been run:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%file ./code/unittest/test_SimpleWidgetTestCase.py\n",
    "\n",
    "import unittest\n",
    "from widget import Widget\n",
    "\n",
    "class SimpleWidgetTestCase(unittest.TestCase):\n",
    "\n",
    "    def setUp(self):\n",
    "        self.widget = Widget('The widget')\n",
    "\n",
    "\n",
    "    def tearDown(self):\n",
    "        self.widget.dispose()\n",
    "\n",
    "    def test_default_widget_size(self):\n",
    "        self.assertEqual(self.widget.size(), (50,50),\n",
    "                         'incorrect default size')\n",
    "\n",
    "    def test_widget_resize(self):\n",
    "        self.widget.resize(100,150)\n",
    "        self.assertEqual(self.widget.size(), (100,150),\n",
    "                         'wrong size after resize')\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    unittest.main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If `setUp()` succeeded, `tearDown()` will be run whether the test method succeeded or not.\n",
    "\n",
    "Such a working environment for the testing code is called a `fixture`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run ./code/unittest/test_SimpleWidgetTestCase.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test Suites\n",
    "\n",
    "<b>Test case instances</b> are grouped together according to the features they test. \n",
    "\n",
    "unittest provides a mechanism for this: <b>the test suite</b>, represented by unittest‘s TestSuite class. \n",
    "\n",
    "In most cases, calling unittest.main() will do the right thing and collect all the module’s test cases for you, and then execute them.\n",
    "\n",
    "However, should you want to customize the building of your test suite, you can do it yourself:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def suite():\n",
    "    \n",
    "    suite = unittest.TestSuite()\n",
    "    \n",
    "    suite.addTest(WidgetTestCase('test_resize'))\n",
    "  \n",
    "　　return suite\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    unittest.main(defaultTest = 'suite')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%file ./code/unittest/test_WidgetTestCase_TestSuite.py\n",
    "\n",
    "import unittest\n",
    "from widget import Widget\n",
    "\n",
    "class WidgetTestCase(unittest.TestCase):\n",
    "\n",
    "    def setUp(self):\n",
    "        self.widget = Widget('The widget')\n",
    "\n",
    "    def tearDown(self):\n",
    "        self.widget.dispose()\n",
    "\n",
    "    def test_default_size(self):\n",
    "        self.assertEqual(self.widget.size(), (50,50),\n",
    "                         'incorrect default size')\n",
    "    def test_resize(self):\n",
    "        self.widget.resize(100,150)\n",
    "        self.assertEqual(self.widget.size(), (100,150),\n",
    "                         'wrong size after resize')\n",
    "\n",
    "def suite():\n",
    "    suite = unittest.TestSuite()\n",
    "    suite.addTest(WidgetTestCase('test_default_size'))\n",
    "    suite.addTest(WidgetTestCase('test_resize'))\n",
    "    return suite\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    unittest.main(defaultTest = 'suite')\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run ./code/unittest/test_WidgetTestCase_TestSuite.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The standard library documentation describes how to organize test suites manually.\n",
    "\n",
    "Automating the construction of test suites is especially useful for large code bases, in which related tests are not all in the same place. Tools such as <b>nose</b> make it easier to manage tests when they are spread over multiple files and directories."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### More:placing the test code in a separate module\n",
    "\n",
    "You can place <b>the definitions of test cases</b> and <b>test suites</b> in the same modules as the code they are to test (such as <b>widget.py</b>),\n",
    "\n",
    "but there are several advantages to <b>placing the test code in a separate module</b>, such as <b>test_*widget*.py</b>:\n",
    "\n",
    "* The test module can be run standalone from the command line.\n",
    "\n",
    "* The test code can more easily be separated from shipped code.\n",
    "\n",
    "* There is less temptation to change test code to fit the code it tests without a good reason.\n",
    "\n",
    "* Test code should be modified much less frequently than the code it tests.\n",
    "\n",
    "* Tested code can be refactored more easily.\n",
    "\n",
    "* Tests for modules written in C must be in separate modules anyway, so why not be consistent?\n",
    "\n",
    "* If the testing strategy changes, there is no need to change the source code.\n",
    "\n",
    "### a GUI tool for test discovery and execution. \n",
    "\n",
    "The script \n",
    "\n",
    "* **Tools/unittestgui/unittestgui.py**\n",
    "\n",
    "in the Python source distribution is a GUI tool for test discovery and execution. This is intended largely for ease of use for those new to unit testing. For production environments it is recommended that tests be driven by a continuous integration system such as Buildbot, Jenkins or Hudson.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## See Also:\n",
    "\n",
    "1. unittest ：https://docs.python.org/3/library/unittest.html  The standard library documentation for this module.\n",
    "\n",
    "2. nose：https://nose.readthedocs.org/en/latest/   A more sophisticated test manager.\n",
    "\n",
    "3. py.test: http://pytest.org/latest   A third-party test runner.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
